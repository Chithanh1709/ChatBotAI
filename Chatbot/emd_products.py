# file: optimized_embed_and_store.py

import json
import os
import hashlib
from sentence_transformers import SentenceTransformer
import chromadb
from tqdm import tqdm
import numpy as np

class ProductEmbedder:
    def __init__(self, chunks_file, db_path, collection_name):
        self.chunks_file = chunks_file
        self.db_path = db_path
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        
        # Load model v·ªõi config t·ªëi ∆∞u
        print("üì• ƒêang t·∫£i m√¥ h√¨nh embedding ti·∫øng Vi·ªát...")
        self.model = SentenceTransformer(
            "keepitreal/vietnamese-sbert",
            device="cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") else "cpu"
        )
    
    def generate_chunk_id(self, text, metadata):
        """T·∫°o ID duy nh·∫•t d·ª±a tr√™n content"""
        content = f"{text}_{metadata.get('product_id', '')}"
        return f"chunk_{hashlib.md5(content.encode()).hexdigest()[:12]}"
    
    def load_and_validate_data(self):
        """ƒê·ªçc v√† validate d·ªØ li·ªáu"""
        print(f"üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {self.chunks_file}...")
        with open(self.chunks_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Validate structure
        validated_data = []
        for i, item in enumerate(data):
            if isinstance(item, dict) and "text" in item:
                validated_data.append(item)
            else:
                print(f"‚ö†Ô∏è  C·∫£nh b√°o: Item {i} kh√¥ng ƒë√∫ng format")
        
        print(f"‚úÖ ƒê√£ validate {len(validated_data)} chunks")
        return validated_data
    
    def encode_in_batches(self, texts, batch_size=32):
        """Encode v·ªõi batch processing t·ªëi ∆∞u"""
        print("üß† ƒêang nh√∫ng vƒÉn b·∫£n...")
        all_embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size)):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(
                batch_texts,
                batch_size=batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True  # Quan tr·ªçng cho cosine similarity
            )
            all_embeddings.extend(batch_embeddings.tolist())
        
        return all_embeddings
    
    def setup_chroma_db(self):
        """Kh·ªüi t·∫°o ChromaDB v·ªõi config t·ªëi ∆∞u"""
        print("üíæ ƒêang thi·∫øt l·∫≠p ChromaDB...")
        self.client = chromadb.PersistentClient(path=self.db_path)
        
        # X√≥a collection c≈© n·∫øu c·∫ßn
        try:
            self.client.delete_collection(name=self.collection_name)
            print("‚ôªÔ∏è  ƒê√£ x√≥a collection c≈©")
        except:
            pass
        
        # T·∫°o collection m·ªõi v·ªõi optimized settings
        self.collection = self.client.create_collection(
            name=self.collection_name,
            metadata={
                "hnsw:space": "cosine",
                "description": "Food products RAG database"
            }
        )
    
    def store_embeddings(self, data, embeddings):
        """L∆∞u embeddings v·ªõi chunking th√¥ng minh"""
        print("üì§ ƒêang l∆∞u embeddings...")
        
        texts = [item["text"] for item in data]
        metadatas = [item["metadata"] for item in data]
        ids = [self.generate_chunk_id(text, metadata) for text, metadata in zip(texts, metadatas)]
        
        # Chunk l·ªõn ƒë·ªÉ tr√°nh memory issues
        chunk_size = 1000
        for i in range(0, len(ids), chunk_size):
            end_idx = min(i + chunk_size, len(ids))
            
            self.collection.add(
                ids=ids[i:end_idx],
                embeddings=embeddings[i:end_idx],
                documents=texts[i:end_idx],
                metadatas=metadatas[i:end_idx]
            )
            print(f"‚úÖ ƒê√£ l∆∞u batch {i//chunk_size + 1}/{(len(ids)-1)//chunk_size + 1}")
    
    def run(self):
        """Ch·∫°y to√†n b·ªô pipeline"""
        # Load data
        data = self.load_and_validate_data()
        
        if not data:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá")
            return
        
        # Encode
        texts = [item["text"] for item in data]
        embeddings = self.encode_in_batches(texts)
        
        # Setup DB
        self.setup_chroma_db()
        
        # Store
        self.store_embeddings(data, embeddings)
        
        # Statistics
        self.print_statistics(data, embeddings)
    
    def print_statistics(self, data, embeddings):
        """In th·ªëng k√™ v·ªÅ d·ªØ li·ªáu"""
        print("\nüìä TH·ªêNG K√ä D·ªÆ LI·ªÜU:")
        print(f"   ‚Ä¢ T·ªïng s·ªë chunks: {len(data)}")
        print(f"   ‚Ä¢ K√≠ch th∆∞·ªõc embedding: {len(embeddings[0])} dimensions")
        
        # Ph√¢n t√≠ch metadata
        product_ids = set()
        categories = set()
        for item in data:
            metadata = item["metadata"]
            product_ids.add(metadata.get('product_id', ''))
            categories.add(metadata.get('category', ''))
        
        print(f"   ‚Ä¢ S·ªë s·∫£n ph·∫©m unique: {len([pid for pid in product_ids if pid])}")
        print(f"   ‚Ä¢ S·ªë categories: {len([c for c in categories if c])}")

# Ch·∫°y pipeline
if __name__ == "__main__":
    embedder = ProductEmbedder(
        chunks_file="rag_chunks.json",
        db_path="D:/chroma_food_rag",
        collection_name="food_products_vn"
    )
    embedder.run()