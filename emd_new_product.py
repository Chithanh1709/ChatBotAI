import json
import hashlib
from sentence_transformers import SentenceTransformer
import chromadb
from tqdm import tqdm
import time
import os

class ProductEmbedder:
    def __init__(self, chunks_file, host='localhost', port=8000, collection_name="food_products_vn"):
        self.chunks_file = chunks_file
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        
        print("üì• ƒêang t·∫£i m√¥ h√¨nh embedding ti·∫øng Vi·ªát...")
        self.model = SentenceTransformer(
            "keepitreal/vietnamese-sbert",
            device="cpu"
        )
    
    def generate_chunk_id(self, text, metadata):
        """T·∫°o ID duy nh·∫•t d·ª±a tr√™n content v√† product_id"""
        product_id = metadata.get('product_id', 'unknown')
        content_hash = hashlib.md5(text.encode()).hexdigest()[:12]
        return f"prod_{product_id}_{content_hash}"
    
    def connect_to_chroma_server(self):
        """K·∫øt n·ªëi t·ªõi ChromaDB server - Gi·ªëng code m·∫´u c·ªßa b·∫°n"""
        try:
            print(f"üîó ƒêang k·∫øt n·ªëi t·ªõi ChromaDB Server ({self.host}:{self.port})...")
            self.client = chromadb.HttpClient(host=self.host, port=self.port)
            self.collection = self.client.get_or_create_collection(name=self.collection_name)
            print(" ƒê√£ k·∫øt n·ªëi t·ªõi ChromaDB Server!")
            return True
        except Exception as e:
            print(f" Kh√¥ng t√¨m th·∫•y Server! L·ªói: {e}")
            print(" B·∫°n ƒë√£ ch·∫°y l·ªánh 'chroma run --host localhost --port 8000' ch∆∞a?")
            return False
    
    def load_and_validate_data(self):
        """ƒê·ªçc v√† validate d·ªØ li·ªáu t·ª´ file JSON"""
        print(f" ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {self.chunks_file}...")
        try:
            with open(self.chunks_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            validated_data = []
            validation_errors = 0
            
            if not isinstance(data, list):
                print(" D·ªØ li·ªáu kh√¥ng ph·∫£i l√† list")
                return []
            
            for i, item in enumerate(data):
                if isinstance(item, dict) and "text" in item and "metadata" in item:
                    metadata = item["metadata"]
                    
                    # Chu·∫©n h√≥a metadata theo c·∫•u tr√∫c c·ªßa b·∫°n
                    standardized_metadata = {
                        "product_id": str(metadata.get("product_id", f"unknown_{i}")),
                        "name": metadata.get("name", "S·∫£n ph·∫©m kh√¥ng t√™n"),
                        "category": metadata.get("category", ""),
                        "unit": metadata.get("unit", ""),
                        "price": metadata.get("price", 0)
                    }
                    
                    validated_item = {
                        "text": item["text"],
                        "metadata": standardized_metadata
                    }
                    validated_data.append(validated_item)
                else:
                    print(f"   Item {i} kh√¥ng ƒë√∫ng format")
                    validation_errors += 1
            
            print(f"‚úÖ ƒê√£ validate {len(validated_data)} chunks (l·ªói: {validation_errors})")
            return validated_data
            
        except Exception as e:
            print(f" L·ªói ƒë·ªçc file: {e}")
            return []
    
    def encode_in_batches(self, texts, batch_size=16):
        """Encode vƒÉn b·∫£n th√†nh embeddings"""
        print(f"üî¢ ƒêang nh√∫ng {len(texts)} vƒÉn b·∫£n...")
        all_embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="Encoding"):
            batch_texts = texts[i:i + batch_size]
            try:
                batch_embeddings = self.model.encode(
                    batch_texts,
                    batch_size=batch_size,
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                all_embeddings.extend(batch_embeddings.tolist())
            except Exception as e:
                print(f" L·ªói encode batch {i//batch_size + 1}: {e}")
                # Fallback: t·∫°o embeddings m·∫∑c ƒë·ªãnh
                embedding_size = 768
                all_embeddings.extend([[0.1] * embedding_size] * len(batch_texts))
        
        return all_embeddings
    
    def store_embeddings(self, data, embeddings):
        """L∆∞u embeddings l√™n ChromaDB server"""
        print(" ƒêang l∆∞u embeddings l√™n server...")
        
        texts = [item["text"] for item in data]
        metadatas = [item["metadata"] for item in data]
        ids = [self.generate_chunk_id(text, metadata) for text, metadata in zip(texts, metadatas)]
        
        # Chia th√†nh c√°c batch nh·ªè ƒë·ªÉ tr√°nh qu√° t·∫£i
        chunk_size = 50
        total_batches = (len(ids) - 1) // chunk_size + 1
        
        successful_batches = 0
        
        for i in range(0, len(ids), chunk_size):
            end_idx = min(i + chunk_size, len(ids))
            batch_num = i // chunk_size + 1
            
            try:
                self.collection.add(
                    ids=ids[i:end_idx],
                    embeddings=embeddings[i:end_idx],
                    documents=texts[i:end_idx],
                    metadatas=metadatas[i:end_idx]
                )
                print(f"‚úÖ ƒê√£ l∆∞u batch {batch_num}/{total_batches} ({end_idx-i} items)")
                successful_batches += 1
                
                # Ngh·ªâ gi·ªØa c√°c batch
                time.sleep(0.5)
                
            except Exception as e:
                print(f" L·ªói l∆∞u batch {batch_num}: {e}")
                # Th·ª≠ l·∫°i v·ªõi batch nh·ªè h∆°n
                if chunk_size > 10:
                    chunk_size = max(10, chunk_size // 2)
                    print(f" Gi·∫£m chunk size xu·ªëng {chunk_size}")
        
        return successful_batches
    
    def verify_data_upload(self):
        """X√°c minh d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c upload th√†nh c√¥ng"""
        try:
            # ƒê·∫øm s·ªë l∆∞·ª£ng documents
            count = self.collection.count()
            print(f"üîç Collection c√≥ {count} documents")
            
            # Th·ª≠ query ƒë∆°n gi·∫£n
            test_results = self.collection.query(
                query_texts=["s·∫£n ph·∫©m"],
                n_results=1
            )
            
            if test_results['documents'] and test_results['documents'][0]:
                print(" X√°c minh th√†nh c√¥ng! D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng.")
                return True
            else:
                print(" Collection tr·ªëng ho·∫∑c kh√¥ng c√≥ k·∫øt qu·∫£")
                return False
                
        except Exception as e:
            print(f" L·ªói x√°c minh: {e}")
            return False
    
    def test_search(self):
        """Test t√¨m ki·∫øm ƒë·ªÉ ƒë·∫£m b·∫£o ho·∫°t ƒë·ªông t·ªët"""
        print("\n ƒêang test t√¨m ki·∫øm...")
        test_queries = ["th·ªãt l·ª£n", "s·ªØa t∆∞∆°i", "b√°nh", "rau"]
        
        for query in test_queries:
            try:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=1
                )
                
                if results['documents'] and results['documents'][0]:
                    doc = results['documents'][0][0]
                    meta = results['metadatas'][0][0]
                    print(f"    '{query}': T√¨m th·∫•y {meta.get('name', 's·∫£n ph·∫©m')}")
                else:
                    print(f"    '{query}': Kh√¥ng t√¨m th·∫•y")
                    
            except Exception as e:
                print(f"    '{query}': L·ªói {e}")
    
    def print_statistics(self, data):
        """In th·ªëng k√™ v·ªÅ d·ªØ li·ªáu"""
        print("\n TH·ªêNG K√ä D·ªÆ LI·ªÜU:")
        print(f"   ‚Ä¢ T·ªïng s·ªë s·∫£n ph·∫©m: {len(data)}")
        
        # Ph√¢n t√≠ch metadata
        product_ids = set()
        categories = set()
        total_price = 0
        price_count = 0
        
        for item in data:
            metadata = item["metadata"]
            product_ids.add(metadata['product_id'])
            categories.add(metadata['category'])
            
            if metadata.get('price', 0) > 0:
                total_price += metadata['price']
                price_count += 1
        
        print(f"   ‚Ä¢ S·ªë s·∫£n ph·∫©m unique: {len(product_ids)}")
        print(f"   ‚Ä¢ S·ªë danh m·ª•c: {len([c for c in categories if c])}")
        
        if price_count > 0:
            avg_price = total_price / price_count
            print(f"   ‚Ä¢ Gi√° trung b√¨nh: {avg_price:,.0f} VNƒê")
        
        # Th·ªëng k√™ ƒë·ªô d√†i text
        text_lengths = [len(item["text"]) for item in data]
        print(f"   ‚Ä¢ ƒê·ªô d√†i m√¥ t·∫£ trung b√¨nh: {sum(text_lengths)/len(text_lengths):.1f} k√Ω t·ª±")
    
    def run(self):
        """Ch·∫°y to√†n b·ªô pipeline nh√∫ng d·ªØ li·ªáu"""
        print(" B·∫Øt ƒë·∫ßu pipeline nh√∫ng d·ªØ li·ªáu v√†o ChromaDB Server...")
        print("=" * 60)
        
        # B∆∞·ªõc 1: K·∫øt n·ªëi server 
        if not self.connect_to_chroma_server():
            return
        
        # B∆∞·ªõc 2: Load v√† validate d·ªØ li·ªáu
        data = self.load_and_validate_data()
        if not data:
            print(" Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ x·ª≠ l√Ω")
            return
        
        # B∆∞·ªõc 3: Encode embeddings
        texts = [item["text"] for item in data]
        embeddings = self.encode_in_batches(texts)
        
        # B∆∞·ªõc 4: L∆∞u l√™n server
        successful_batches = self.store_embeddings(data, embeddings)
        
        # B∆∞·ªõc 5: X√°c minh
        if successful_batches > 0:
            print("\nüîç ƒêang x√°c minh d·ªØ li·ªáu...")
            if self.verify_data_upload():
                self.test_search()
        
        # B∆∞·ªõc 6: Th·ªëng k√™
        self.print_statistics(data)
        
        print(f"\n HO√ÄN TH√ÄNH! ƒê√£ nh√∫ng {len(data)} s·∫£n ph·∫©m v√†o ChromaDB Server!")

# Ch·∫°y pipeline
if __name__ == "__main__":
    # C·∫•u h√¨nh 
    CHUNKS_FILE = "rag_chunks_new.json"
    CHROMA_HOST = "localhost"
    CHROMA_PORT = 8000
    COLLECTION_NAME = "food_products_vn"
    
    # Ki·ªÉm tra file
    if not os.path.exists(CHUNKS_FILE):
        print(f" File {CHUNKS_FILE} kh√¥ng t·ªìn t·∫°i!")
        exit(1)
    
    # Ki·ªÉm tra k√≠ch th∆∞·ªõc file
    file_size = os.path.getsize(CHUNKS_FILE)
    if file_size == 0:
        print(f" File {CHUNKS_FILE} r·ªóng!")
        exit(1)
    
    print(f" File d·ªØ li·ªáu: {CHUNKS_FILE} ({file_size} bytes)")
    
    # Ch·∫°y embedder
    embedder = ProductEmbedder(
        chunks_file=CHUNKS_FILE,
        host=CHROMA_HOST,
        port=CHROMA_PORT,
        collection_name=COLLECTION_NAME
    )
    
    embedder.run()